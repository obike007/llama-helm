name: CI/CD Pipeline for LLaMA Server

on:
  push:
    branches: [ main, master ]
    tags: [ 'v*' ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:

env:
  # Configure for Docker Hub
  REGISTRY: docker.io
  IMAGE_NAME: ${{ secrets.DOCKER_HUB_USERNAME }}/llama-server

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      # Docker Hub login using secrets
      - name: Log into Docker Hub
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ secrets.DOCKER_HUB_USERNAME }}
          password: ${{ secrets.DOCKER_HUB_TOKEN }}

      # Extract metadata for the Docker image
      - name: Extract Docker metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=ref,event=branch
            type=sha,prefix=sha-,format=short

      # Build and push Docker image
      - name: Build and push Docker image
        id: build-and-push
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: ${{ github.event_name != 'pull_request' }}
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          no-cache: true  # Disable cache to avoid previous issues

      # Install kubectl and kind
      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.28.0'

      - name: Install Kind
        uses: helm/kind-action@v1.8.0
        with:
          install_only: true

      - name: Create Kind cluster
        run: |
          chmod +x ./scripts/setup-kind.sh
          ./scripts/setup-kind.sh
          
          # Verify storage class was created
          echo "Checking storage classes..."
          kubectl get storageclass

      - name: Install Helm
        uses: azure/setup-helm@v3
        with:
          version: 'v3.12.3'

      # Add Helm repos
      - name: Add Helm repositories
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update

      # Deploy minimal monitoring stack with simplified values
      - name: Deploy monitoring
        run: |
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
          
          # Create simplified values for Prometheus
          cat > monitoring/prometheus-values-ci.yaml << EOF
          alertmanager:
            enabled: false
            
          kubeStateMetrics:
            enabled: false
            
          nodeExporter:
            enabled: false
            
          prometheusOperator:
            admissionWebhooks:
              enabled: false
            resources:
              limits:
                cpu: 100m
                memory: 128Mi
              requests:
                cpu: 50m
                memory: 64Mi
                
          prometheus:
            enabled: true
            service:
              type: NodePort
              nodePort: 30900
            prometheusSpec:
              retention: 1h
              resources:
                limits:
                  cpu: 250m
                  memory: 256Mi
                requests:
                  cpu: 100m
                  memory: 128Mi
                  
          grafana:
            enabled: true
            adminPassword: "admin"
            service:
              type: NodePort
              nodePort: 30300
            resources:
              limits:
                cpu: 100m
                memory: 128Mi
              requests:
                cpu: 50m
                memory: 64Mi
          EOF
          
          # Try installation with retries
          for i in {1..2}; do
            echo "Attempt $i to install Prometheus..."
            if helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
              --namespace monitoring \
              --values monitoring/prometheus-values-ci.yaml \
              --timeout 5m; then
              echo "Prometheus installed successfully"
              break
            else
              echo "Prometheus installation attempt $i failed"
              # Continue anyway after last attempt
              if [ $i -eq 2 ]; then
                echo "Continuing with deployment despite Prometheus installation failure"
              else
                sleep 10
              fi
            fi
          done

      # Fetch built image to Kind
      - name: Load Docker image to Kind
        run: |
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            # For PRs, build locally
            echo "Building local image for pull request..."
            docker build -t ${{ secrets.DOCKER_HUB_USERNAME }}/llama-server:pr-${{ github.event.pull_request.number || 'local' }} .
            kind load docker-image ${{ secrets.DOCKER_HUB_USERNAME }}/llama-server:pr-${{ github.event.pull_request.number || 'local' }} --name llama-cluster
            echo "IMAGE_TAG=pr-${{ github.event.pull_request.number || 'local' }}" >> $GITHUB_ENV
          else
            # For pushes, build locally and use the SHA tag
            echo "Building local image for deployment..."
            SHA_SHORT=$(echo "${{ github.sha }}" | cut -c1-7)
            docker build -t ${{ secrets.DOCKER_HUB_USERNAME }}/llama-server:$SHA_SHORT .
            kind load docker-image ${{ secrets.DOCKER_HUB_USERNAME }}/llama-server:$SHA_SHORT --name llama-cluster
            echo "IMAGE_TAG=$SHA_SHORT" >> $GITHUB_ENV
          fi

      # Test a simple PVC creation
      - name: Test PVC creation
        run: |
          echo "Testing PVC creation..."
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: PersistentVolumeClaim
          metadata:
            name: test-pvc
            namespace: default
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 100Mi
          EOF
          
          kubectl get pvc test-pvc
          
      # Deploy app using Helm with simplified model setup
      - name: Deploy LLaMA server with Helm
        run: |
          kubectl create namespace llama --dry-run=client -o yaml | kubectl apply -f -
          
          # For CI/CD, use a simplified deployment without actual model
          helm upgrade --install llama-server ./helm/llama-server \
            --namespace llama \
            --set image.repository=${{ secrets.DOCKER_HUB_USERNAME }}/llama-server \
            --set image.tag=${{ env.IMAGE_TAG }} \
            --set image.pullPolicy=IfNotPresent \
            --set modelConfig.downloadModel=false \
            --set persistence.enabled=true \
            --set persistence.size=100Mi \
            --set service.type=NodePort \
            --timeout 5m

          # Create an empty model file with ConfigMap
          echo "Creating dummy model file..."
          cat <<EOF | kubectl -n llama apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: dummy-model
          data:
            model.gguf: |
              DUMMY_MODEL_DATA_FOR_TESTING_ONLY
          EOF
          
          # Create a job to copy the model file to PVC
          cat <<EOF | kubectl -n llama apply -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: model-setup
          spec:
            ttlSecondsAfterFinished: 100
            template:
              spec:
                containers:
                - name: model-setup
                  image: busybox
                  command: ["sh", "-c", "cp /dummy/model.gguf /models/ && echo 'Dummy model copied'"]
                  volumeMounts:
                  - name: dummy-model
                    mountPath: /dummy
                  - name: models
                    mountPath: /models
                volumes:
                - name: dummy-model
                  configMap:
                    name: dummy-model
                - name: models
                  persistentVolumeClaim:
                    claimName: llama-server-models
                restartPolicy: Never
            backoffLimit: 2
          EOF
          
          # Wait for job completion
          echo "Waiting for model setup job to complete..."
          kubectl -n llama wait --for=condition=complete --timeout=1m job/model-setup

      # Monitor the deployment
      - name: Monitor deployment
        run: |
          echo "Checking deployment status..."
          kubectl -n llama get deployment
          
          echo "Checking pod status..."
          kubectl -n llama get pods
          
          echo "Checking pod events..."
          POD_NAME=$(kubectl -n llama get pods -l app.kubernetes.io/name=llama-server -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "pod-not-found")
          if [ "$POD_NAME" != "pod-not-found" ]; then
            kubectl -n llama describe pod $POD_NAME
          fi

      # Verify deployment with extended wait time
      - name: Verify deployment
        run: |
          echo "Waiting for LLaMA server to be ready..."
          kubectl -n llama rollout status deployment/llama-server --timeout=5m || true
          
          # Even if the rollout status command failed, check if the pods are running
          echo "Current pod status:"
          kubectl -n llama get pods
          
          # Check if at least one pod is running
          RUNNING_PODS=$(kubectl -n llama get pods -l app.kubernetes.io/name=llama-server --field-selector=status.phase=Running -o name | wc -l)
          if [ "$RUNNING_PODS" -gt 0 ]; then
            echo "At least one pod is running, considering deployment successful"
          else
            # Get logs from the failing pod
            POD_NAME=$(kubectl -n llama get pods -l app.kubernetes.io/name=llama-server -o jsonpath='{.items[0].metadata.name}')
            echo "Checking logs from pod $POD_NAME:"
            kubectl -n llama logs $POD_NAME --all-containers || true
            echo "Deployment failed - no running pods found"
            exit 1
          fi
          
          echo "Services available:"
          kubectl -n llama get svc
          
      - name: Get service URLs
        if: success()
        run: |
          echo "LLaMA Server is available at: http://localhost:30080"
          echo "Metrics endpoint is available at: http://localhost:30090/metrics"
          echo "Prometheus UI is available at: http://localhost:30900"
          echo "Grafana is available at: http://localhost:30300 (admin/admin)"